{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronales Netz zum erstellen eines Deepfakes\n",
    "***\n",
    "\n",
    "Dieses Dokument ermöglicht Dir, ein Deepfake-Video zu erstellen, wären Du über die Funktion und den Aufbau des dazu verwendeten künstlichen Neuronalen Netzes lernst. \n",
    "\n",
    "Es müssen zum Ausführen des Codes folgende Python Module installiert sein: \n",
    "\n",
    "***\n",
    "##### *Das folgende Dokument setzt sich aus folgenden Schritten zusammen:*\n",
    "1. Einlesen und Aufbereiten von den Daten zum Trainieren\n",
    "    - Extrahieren von Gesichter aus einem Video\n",
    "    - Einlesen dieser Daten\n",
    "2. Initialisieren des künstlichen Neuronalen Netzes\n",
    "    - Definieren der Struktur des Encoder und Decoder\n",
    "    - Kombinieren des Encoder und Decoder zu den Autoencodern\n",
    "    - Kompilieren der Modelle mit einem Optimizer\n",
    "3. Trainieren\n",
    "    - Festlegen von Checkpoints für das Trainieren\n",
    "    - Eigentliches Trainieren des Netzes\n",
    "4. Fälschen eines neuen Videos\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einlesen und Aufbereiten von den Daten zum Trainieren\n",
    "***\n",
    "Um dem künstlichen Neuronalen Netz beizubringen Gesichter zu erkennen und zu erstellen, werden zahlreiche Bilder von Gesichtern benötigt. Im Fall von Deepfakes ist Bildmaterial von zwei Personen nötig. Einfachheitshalber wird als Input Videomaterial verwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrahieren von Gesichter aus einem Video\n",
    "In den Variablen *PFAD_A* und *PFAD_B* werden die Pfade zu den Videos gespeichert, die dazu verwendet werden Bilder von zwei Personen zu erxtrahieren. Es werden mit der Hilfe der Klasse `Gesichterextrahierer`, die in einer externen Datei definiert wurde, die Gesichter aus den Videos extrahiert. Die Bilder werden in einem Verzeichnis gespeichert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import Gesichterextrahierer as GE\n",
    "\n",
    "PFAD_A = './daten/videomaterial/Joe_Biden/nur_joe_biden_gemischt.mp4'\n",
    "PFAD_B = './daten/videomaterial/Chuck_Norris/nur_Chuck_Norris_gemischt.mp4'\n",
    "PFAD_KASKADE = './daten/cascades/haarcascade_frontalface_default.xml'\n",
    "\n",
    "g = GE.Gesichterextrahierer(PFAD_KASKADE)\n",
    "g.lade(PFAD_A)\n",
    "g.extrahiereGesichter(\n",
    "    max_anzahl_bilder=3000,\n",
    "    ordner_ausgabe='./daten/lernen/Gesichter/A'\n",
    ")\n",
    "g.lade(PFAD_B)\n",
    "g.extrahiereGesichter(\n",
    "    max_anzahl_bilder=3000,\n",
    "    ordner_ausgabe='./daten/lernen/Gesichter/B'\n",
    ")\n",
    "\n",
    "del PFAD_A, PFAD_B, PFAD_KASKADE, g, GE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einlesen der Bilddateien\n",
    "Dass die Bilder als Datei gespeichert wurden, erspart uns beim nächsten Mal den vorherigen Schritt. Die Dateien müssen nun allerdings wieder ins Programm geladen werden.\n",
    "\n",
    "> `erstelleDatensatz(pfad: str) -> list[list]`\n",
    ">\n",
    "> Lädt alle Bilder in dem übergebenen Verzeichnis in zwei Datensätze und gibt diese als Liste zurück. Jeder Pixelwert wird durch 255 geteilt, um die Werte auf den Bereich zwischen 0 und 1 zu projektieren. Dies stellt sicher, dass die Werte des künstlischen Neuronale Netzes (KNN), wenn das Bild übergeben wird, nicht zu groß werden. Die Bilder werden in zwei Datensätze umgewandelt, zu 75% zum Trainieren des KNN und zu 25% zum Prüfen und Bewerten der Leistung des Netzes.\n",
    "\n",
    "> `def teileListe(liste: list, verteilung: float) -> list[list]`\n",
    ">\n",
    "> Teilt die übergebene Liste in zwei Listen und gibt diese zurück. Die erste zurückgegebene Liste hat ein Länge von _n_-% der übergebenen Liste, wobei _n_ als Kommazahl zwischen 1 und 0 mit `verteilung` übergeben wird. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19000 Bilder aus daten/lernen/Gesichter/A geladen.\n",
      "19000 Bilder aus daten/lernen/Gesichter/B geladen.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def erstelleDatensatz(pfad: str, anzahl: int) -> list[list]:\n",
    "    bilder = []\n",
    "    for wurzel, ordner, dateien in os.walk(pfad):\n",
    "        for datei in dateien:\n",
    "            if datei.split(\".\")[-1].lower() not in ['png', 'jpg', 'jpeg']: continue\n",
    "            bild = cv2.imread(os.path.join(wurzel, datei))\n",
    "            bild = cv2.pyrDown(bild)\n",
    "            bild = bild.astype('float32')\n",
    "            bild /= 255.0\n",
    "            bilder.append(bild)\n",
    "            if len(bilder) >= anzahl: break\n",
    "            bilder.append(np.fliplr(bild))\n",
    "            if len(bilder) >= anzahl: break\n",
    "    np.random.shuffle(bilder)\n",
    "    bilder_train, bilder_test = teileListe(bilder, 0.75)\n",
    "    bilder_train, bilder_test = np.array(bilder_train), np.array(bilder_test)\n",
    "    print('%d Bilder aus %s geladen.' % (len(bilder), pfad))\n",
    "    return [bilder_train, bilder_test]\n",
    "\n",
    "def teileListe(liste: list, verteilung: float) -> list[list]:\n",
    "    x = int(len(liste)*verteilung)\n",
    "    return [liste[:x], liste[x:]]\n",
    "\n",
    "def verzerren(bild: list, staerke: int):\n",
    "    hoehe = bild.shape[0]\n",
    "    breite = bild.shape[1]\n",
    "    punkte_von = np.float32([[0, 0], [0, hoehe], [breite, 0], [breite, hoehe]])\n",
    "    punkte_nach = np.float32([[0, staerke], [0, hoehe-staerke], [breite, 0], [breite, hoehe]])\n",
    "    matrix = cv2.getPerspectiveTransform(punkte_von, punkte_nach)\n",
    "    bild_verzerrt = cv2.warpPerspective(bild, matrix, (breite, hoehe))\n",
    "    bild_verzerrt = bild_verzerrt[staerke:hoehe-staerke, staerke:breite-staerke]\n",
    "    return cv2.resize(bild_verzerrt, (breite, hoehe))\n",
    "\n",
    "datensatz_gesichter_A_train, datensatz_gesichter_A_test = erstelleDatensatz('daten/lernen/Gesichter/A', 19000)\n",
    "NAME_AUTOENCODER_A = 'Biden'\n",
    "\n",
    "datensatz_gesichter_B_train, datensatz_gesichter_B_test = erstelleDatensatz('daten/lernen/Gesichter/B', 19000)\n",
    "NAME_AUTOENCODER_B = 'Norris'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Um zu prüfen, ob dies geklappt hat, wird hier ein beispielhaftes Bild dargestellt.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "\n",
    "imshow(cv2.cvtColor(datensatz_gesichter_B_test[6], cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisieren des Neuronalen Netzes\n",
    "***\n",
    "Nun wird das künstlichen Neuronale Netz initialisiert. Dazu wird die Struktur des Netzes definiert und das Modell anschließend kompiliert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definieren der Struktur des Encoder und Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "IMG_SHAPE = (64, 64, 3)\n",
    "NAME = \"DenseLayer_für_die_obduktion\"\n",
    "\n",
    "def logSummary(string: str):\n",
    "    with open(f\"./daten/modelle/{NAME}/modell.info\", \"a\") as datei:\n",
    "        datei.write(string + \"\\n\")\n",
    "\n",
    "def gibEncoder():\n",
    "    encoder = tf.keras.Sequential(name='encoder')\n",
    "    encoder.add(tf.keras.layers.Reshape((12288,), input_shape=IMG_SHAPE ))\n",
    "    encoder.add(tf.keras.layers.Dense(800))\n",
    "    #encoder.add(tf.keras.layers.Conv2D(32, kernel_size=5, strides=2, padding='same', input_shape=( IMG_SHAPE ) ))\n",
    "    #encoder.add(tf.keras.layers.Conv2D(64, kernel_size=5, strides=2, padding='same'))\n",
    "    #encoder.add(tf.keras.layers.Conv2D(128, kernel_size=5, strides=2, padding='same'))\n",
    "    #encoder.add(tf.keras.layers.Conv2D(256, kernel_size=5, strides=2, padding='same'))\n",
    "    #encoder.add(tf.keras.layers.Flatten())\n",
    "    encoder.add(tf.keras.layers.Dense( 200 ))\n",
    "    \n",
    "    encoder.summary(print_fn=logSummary)\n",
    "    print(encoder.summary())\n",
    "    return encoder\n",
    "\n",
    "def gibDecoder():\n",
    "    decoder = tf.keras.Sequential(name='decoder')\n",
    "    decoder.add(tf.keras.layers.Dense(800, input_shape=(200,) ))\n",
    "    decoder.add(tf.keras.layers.Dense(12288))\n",
    "    decoder.add(tf.keras.layers.Reshape(IMG_SHAPE))\n",
    "    #decoder.add(tf.keras.layers.Dense( (8*8*256), input_shape=(500,)))\n",
    "    #decoder.add(tf.keras.layers.Reshape( (8, 8, 256) ))\n",
    "    #decoder.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=5, strides=2, padding='same'))\n",
    "    #decoder.add(tf.keras.layers.Conv2DTranspose(128, kernel_size=5, strides=2, padding='same'))\n",
    "    #decoder.add(tf.keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2, padding='same'))\n",
    "    #decoder.add(tf.keras.layers.Conv2DTranspose(32, kernel_size=5, strides=2, padding='same'))\n",
    "    #decoder.add(tf.keras.layers.Conv2DTranspose(3, kernel_size=1))\n",
    "    \n",
    "    decoder.summary(print_fn=logSummary)\n",
    "    print(decoder.summary())\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kombinieren des Encoder und Decoder zu den Autoencodern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gibAutoencoder(name):\n",
    "    x = tf.keras.layers.Input( shape=IMG_SHAPE, name='input_layer' )\n",
    "    encoder, decoder = gibEncoder(), gibDecoder()\n",
    "    autoencoder = tf.keras.Model(x, decoder(encoder(x)), name=name)\n",
    "    \n",
    "    print(autoencoder.summary())\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kompilieren der Modelle mit einem Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER_FUNKTION = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "LOSS_FUNKTION = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def gibKompiliertenAutoencoder(name):\n",
    "    autoencoder = gibAutoencoder(name)\n",
    "    autoencoder.compile(optimizer=OPTIMIZER_FUNKTION, loss=LOSS_FUNKTION)\n",
    "    \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelle von der Festplatte geladen.\n",
      "\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 4.8972e-04\n",
      "Aktueller Loss von A (Biden): 0.0004897157195955515\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 7.8433e-04\n",
      "Aktueller Loss von B (Norris): 0.0007843341445550323\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    autoencoder_A = tf.keras.models.load_model(f\"./daten/modelle/{NAME}/{NAME_AUTOENCODER_A}/\")\n",
    "    autoencoder_B = tf.keras.models.load_model(f\"./daten/modelle/{NAME}/{NAME_AUTOENCODER_B}/\")\n",
    "    print(\"Modelle von der Festplatte geladen.\\n\")\n",
    "except:\n",
    "    try:\n",
    "        os.mkdir(f\"./daten/modelle/{NAME}/\")\n",
    "        os.mkdir(f\"./daten/modelle/{NAME}/{NAME_AUTOENCODER_A}/\")\n",
    "        os.mkdir(f\"./daten/modelle/{NAME}/{NAME_AUTOENCODER_B}/\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    autoencoder_A = gibKompiliertenAutoencoder(name=\"autoencoder_A\")\n",
    "    autoencoder_B = gibKompiliertenAutoencoder(name=\"autoencoder_B\")\n",
    "    \n",
    "loss = autoencoder_A.evaluate(datensatz_gesichter_A_test[:32], datensatz_gesichter_A_test[:32])\n",
    "print(f\"Aktueller Loss von A ({NAME_AUTOENCODER_A}): {loss}\")\n",
    "\n",
    "loss = autoencoder_B.evaluate(datensatz_gesichter_B_test[:32], datensatz_gesichter_B_test[:32])\n",
    "print(f\"Aktueller Loss von B ({NAME_AUTOENCODER_B}): {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainieren\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Festlegen von Checkpoints für das Trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import LoggingCallback as lc\n",
    "\n",
    "\n",
    "autoencoder_A_logging_callback = lc.LoggingCallback(\n",
    "    pfad_modell=f\"./daten/modelle/{NAME}/{NAME_AUTOENCODER_A}/\",\n",
    "    bild_A=datensatz_gesichter_A_test[1],\n",
    "    bild_B=datensatz_gesichter_B_test[1]\n",
    ")\n",
    "\n",
    "autoencoder_B_logging_callback = lc.LoggingCallback(\n",
    "    pfad_modell=f\"./daten/modelle/{NAME}/{NAME_AUTOENCODER_B}/\",\n",
    "    bild_A=datensatz_gesichter_A_test[1],\n",
    "    bild_B=datensatz_gesichter_B_test[1]\n",
    ")\n",
    "\n",
    "autoencoder_A_checkpoint_callback = ModelCheckpoint(\n",
    "    f\"./daten/modelle/{NAME}/{NAME_AUTOENCODER_A}/\",\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "autoencoder_B_checkpoint_callback = ModelCheckpoint(\n",
    "    f\"./daten/modelle/{NAME}/{NAME_AUTOENCODER_B}/\",\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigentliches Trainieren des Netzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, gc\n",
    "\n",
    "ZEITPUNKT_ENDE = time.time() + int(7.8*60*60)\n",
    "\n",
    "while time.time() < ZEITPUNKT_ENDE:\n",
    "    print(\"!- Noch für ~{:.2f}h beschäftigt.\".format(( ZEITPUNKT_ENDE-time.time() )/3600) )\n",
    "    \n",
    "    autoencoder_A.fit(\n",
    "                      datensatz_gesichter_A_train,\n",
    "                      datensatz_gesichter_A_train,\n",
    "                      epochs=5,\n",
    "                      batch_size=32,\n",
    "                      shuffle=True,\n",
    "                      validation_data=(datensatz_gesichter_A_test, datensatz_gesichter_A_test),\n",
    "                      callbacks=[autoencoder_A_checkpoint_callback, autoencoder_A_logging_callback]\n",
    "                     )\n",
    "    autoencoder_B.layers[1] = autoencoder_A.get_layer('encoder')\n",
    "    gc.collect()\n",
    "    \n",
    "    autoencoder_B.fit(\n",
    "                      datensatz_gesichter_B_train,\n",
    "                      datensatz_gesichter_B_train,\n",
    "                      epochs=5,\n",
    "                      batch_size=32,\n",
    "                      shuffle=True,\n",
    "                      validation_data=(datensatz_gesichter_B_test, datensatz_gesichter_B_test),\n",
    "                      callbacks=[autoencoder_B_checkpoint_callback, autoencoder_B_logging_callback]\n",
    "                     )\n",
    "    autoencoder_A.layers[1] = autoencoder_B.get_layer('encoder')\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(cv2.cvtColor(autoencoder_A.predict(datensatz_gesichter_B_test[4].reshape(1, 64, 64, 3))[0], cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Gesichterextrahierer as GE\n",
    "\n",
    "PFAD_KASKADE = './daten/cascades/haarcascade_frontalface_default.xml'\n",
    "\n",
    "def fake(num_bild_tuple):\n",
    "    global autoencoder_B\n",
    "    i, bild = num_bild_tuple\n",
    "    bild = bild.astype('float32')\n",
    "    bild /= 255.0\n",
    "    erg = autoencoder_B.predict(bild.reshape(1, 128, 128, 3))[0]\n",
    "    erg *= 255.0\n",
    "    return erg\n",
    "\n",
    "g = GE.Gesichterextrahierer(PFAD_KASKADE)\n",
    "g.lade('./Biden.mp4')\n",
    "g.fuerGesichterMache(fake, 10000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_A.save(f\"./daten/modelle/{NAME}/{NAME_AUTOENCODER_A}/gewichtungen.h5\")\n",
    "autoencoder_B.save(f\"./daten/modelle/{NAME}/{NAME_AUTOENCODER_B}/gewichtungen.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
